# -*- coding: utf-8 -*-
"""keratry.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RZloTFgE0pjovpGxixBFX2ZYW4B4WeaL
"""

import nltk
nltk.download('abc')
nltk.download('punkt')

!pip install Keras==2.0.5
!pip install tensorflow==1.13.2

from keras.models import Model
from keras.layers import Input, Dense, Reshape, merge
from keras.layers.embeddings import Embedding
import pickle
import os

def savetodrive(name):
  from pydrive.auth import GoogleAuth
  from pydrive.drive import GoogleDrive
  from google.colab import auth
  from oauth2client.client import GoogleCredentials

  # 1. Authenticate and create the PyDrive client.
  auth.authenticate_user()
  gauth = GoogleAuth()
  gauth.credentials = GoogleCredentials.get_application_default()
  drive = GoogleDrive(gauth)  

  # get the folder id where you want to save your file
  file = drive.CreateFile({'parents':[{u'id': '1oiIOV1lkwsbi78IfOMRNxWUTXtWceKgF'}]})
  file.SetContentFile(name)
  file.Upload() 
  print("FILE SAVED   " + name)

import re
import numpy as np
import string

def getOneHot(index, count):
    wordVec = numpy.zeros(count, dtype = int)
    wordVec[index] = 1
    return wordVec

# sents = [['he', 'is', 'the', 'king', '.'], ['the', 'king', 'is', 'royal', '?'], ['she', 'is', 'the', 'royal', 'queen']]
sents = list(nltk.corpus.abc.sents())

#conversion to lower case
for i in range(len(sents)):
    for j in range(len(sents[i])):
        sents[i][j] = sents[i][j].lower()

#creating dictionary of all the words
wordDict = {}
for i in sents:
    for j in i:
        #removal of punctuation
        if j not in string.punctuation:
            if j in wordDict:
                wordDict[j] += 1
            else:   
                wordDict[j] = 1
print(wordDict)

#creating word indices
wordIndex = {}
index = 0
for i in wordDict.keys():
    if i not in wordIndex:
        wordIndex[i] = index
        index += 1
print(wordIndex)

reverse_dictionary = dict(zip(wordIndex.values(), wordIndex.keys()))

vocab_size = len(wordDict)
windowSize = 2

trainingData = []  
for i in sents:
    for j in range(len(i)):
        # print(j, i[j])
        if i[j] in wordDict.keys():
            curWord = wordIndex[i[j]]#getOneHot(wordIndex[i[j]], vocabSize)
            contextWords = []
            for k in range(j - windowSize, j + windowSize + 1):
                if j != k and k <= len(i) - 1 and k >= 0:
                    if i[k] in wordDict.keys():
                        contextWords.append(wordIndex[i[k]])#(getOneHot(wordIndex[i[k]], vocabSize))
            trainingData.append([curWord, contextWords])           

print(trainingData)

xTrain = []
yTrain = []
labels = []
for i in trainingData:
    # print(i[0])
    for j in i[1]:
        xTrain.append(i[0])
        yTrain.append(j)
        labels.append(1)

word_target = np.array(xTrain)
word_context = np.array(yTrain)
print("TRAINERS READY")

window_size = 2
vector_dim = 300
epochs = 200000
print("TARGET", word_target)
print("CONTEXT", word_context)

# create some input variables
input_target = Input((1,))
input_context = Input((1,))

embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')
target = embedding(input_target)
target = Reshape((vector_dim, 1))(target)
context = embedding(input_context)
context = Reshape((vector_dim, 1))(context)

# now perform the dot product operation to get a similarity measure
dot_product = merge([target, context], mode='dot', dot_axes=1)
dot_product = Reshape((1,))(dot_product)

# add the sigmoid output layer
output = Dense(1, activation='sigmoid')(dot_product)

# create the primary training model
model = Model(input=[input_target, input_context], output=output)
model.compile(loss='binary_crossentropy', optimizer='rmsprop')

for cnt in range(epochs):
    index = np.random.randint(0, len(word_target)-1)
    target1 = []
    context1 = []
    label1 = []
    target1.append(word_target[index])
    context1.append(word_context[index])
    label1.append(labels[index])
    loss = model.train_on_batch([np.array(target1), np.array(context1)], np.array(label1))
    # print(model.get_layer(name='embedding').get_weights()[0].shape)
    if cnt % 10000 == 0:
      pickle_w = open("modelsnew120"+str(cnt), "wb")
      pickle.dump([model.get_layer(name='embedding').get_weights()[0], wordIndex, wordDict],pickle_w)
      savetodrive("modelsnew120" + str(cnt)) 
    if cnt % 100 == 0:
        # print(model.get_layer(name='embedding').get_weights()[0].shape)
        print("Iteration {}, loss={}".format(cnt, loss))
# pickle_w = open("models20end", "wb")
# pickle.dump([model.get_layer(name='embedding').get_weights()[0], wordIndex, wordDict],pickle_w)
# savetodrive("models20end")

# import os 
# curList = os.listdir(os.getcwd())
# for i in curList:
#   if "models20" in i:
#     os.remove(i)
